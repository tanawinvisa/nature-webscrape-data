{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanawinvisa/nature-webscrape-data/blob/main/DataForNetworkViz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "33X1ZG43gtwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2ejPnWKgXbx",
        "outputId": "61f015b9-fda4-488d-c2c7-2bc01f7e5144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-06 19:53:43--  https://github.com/nnatchy/DSDE_Project/raw/main/2018_test.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/nnatchy/DSDE_Project/main/2018_test.zip [following]\n",
            "--2024-05-06 19:53:43--  https://raw.githubusercontent.com/nnatchy/DSDE_Project/main/2018_test.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6913340 (6.6M) [application/zip]\n",
            "Saving to: ‘2018.zip’\n",
            "\n",
            "2018.zip            100%[===================>]   6.59M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-05-06 19:53:43 (241 MB/s) - ‘2018.zip’ saved [6913340/6913340]\n",
            "\n",
            "--2024-05-06 19:53:43--  https://github.com/nnatchy/DSDE_Project/raw/main/2019_test.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/nnatchy/DSDE_Project/main/2019_test.zip [following]\n",
            "--2024-05-06 19:53:44--  https://raw.githubusercontent.com/nnatchy/DSDE_Project/main/2019_test.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7328143 (7.0M) [application/zip]\n",
            "Saving to: ‘2019.zip’\n",
            "\n",
            "2019.zip            100%[===================>]   6.99M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-05-06 19:53:44 (205 MB/s) - ‘2019.zip’ saved [7328143/7328143]\n",
            "\n",
            "--2024-05-06 19:53:44--  https://github.com/nnatchy/DSDE_Project/raw/main/2020_test.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/nnatchy/DSDE_Project/main/2020_test.zip [following]\n",
            "--2024-05-06 19:53:44--  https://raw.githubusercontent.com/nnatchy/DSDE_Project/main/2020_test.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7393768 (7.1M) [application/zip]\n",
            "Saving to: ‘2020.zip’\n",
            "\n",
            "2020.zip            100%[===================>]   7.05M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-05-06 19:53:44 (150 MB/s) - ‘2020.zip’ saved [7393768/7393768]\n",
            "\n",
            "--2024-05-06 19:53:44--  https://github.com/nnatchy/DSDE_Project/raw/main/2021_test.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/nnatchy/DSDE_Project/main/2021_test.zip [following]\n",
            "--2024-05-06 19:53:45--  https://raw.githubusercontent.com/nnatchy/DSDE_Project/main/2021_test.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7509824 (7.2M) [application/zip]\n",
            "Saving to: ‘2021.zip’\n",
            "\n",
            "2021.zip            100%[===================>]   7.16M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-05-06 19:53:45 (225 MB/s) - ‘2021.zip’ saved [7509824/7509824]\n",
            "\n",
            "--2024-05-06 19:53:45--  https://github.com/nnatchy/DSDE_Project/raw/main/2022_test.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/nnatchy/DSDE_Project/main/2022_test.zip [following]\n",
            "--2024-05-06 19:53:45--  https://raw.githubusercontent.com/nnatchy/DSDE_Project/main/2022_test.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7154957 (6.8M) [application/zip]\n",
            "Saving to: ‘2022.zip’\n",
            "\n",
            "2022.zip            100%[===================>]   6.82M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-05-06 19:53:45 (185 MB/s) - ‘2022.zip’ saved [7154957/7154957]\n",
            "\n",
            "--2024-05-06 19:53:45--  https://github.com/nnatchy/DSDE_Project/raw/main/2023_test.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/nnatchy/DSDE_Project/main/2023_test.zip [following]\n",
            "--2024-05-06 19:53:45--  https://raw.githubusercontent.com/nnatchy/DSDE_Project/main/2023_test.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7874655 (7.5M) [application/zip]\n",
            "Saving to: ‘2023.zip’\n",
            "\n",
            "2023.zip            100%[===================>]   7.51M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-05-06 19:53:46 (146 MB/s) - ‘2023.zip’ saved [7874655/7874655]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "years = range(2018, 2024)  # Years from 2018 to 2023\n",
        "for year in years:\n",
        "    # Construct the URL\n",
        "    url = f\"https://github.com/nnatchy/DSDE_Project/raw/main/{year}_test.zip\"\n",
        "    # Construct the wget command to download and rename the file directly\n",
        "    !wget {url} -O {year}.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_files(year_range, base_path):\n",
        "    batch_years = []\n",
        "    for year in year_range:\n",
        "        # Determine the directory pattern based on the year\n",
        "        year_directory = f\"{base_path}/{year}/\" if year == 2018 else f\"{base_path}/{year}/\"\n",
        "\n",
        "        # Ensure the year directory exists and create if not\n",
        "        os.makedirs(year_directory, exist_ok=True)\n",
        "\n",
        "        # Check for zip files in the base path and unzip them to the year directory\n",
        "        zip_file_path = os.path.join(base_path, f'{year}.zip')\n",
        "        if os.path.exists(zip_file_path):\n",
        "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(year_directory)\n",
        "                print(f\"Extracted {zip_file_path} to {year_directory}\")\n",
        "\n",
        "by = process_files(range(2018, 2024), \".\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCCPAUkKhYZN",
        "outputId": "9cd91861-b6c6-4905-a6b3-3f5208a7f2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted ./2018.zip to ./2018/\n",
            "Extracted ./2019.zip to ./2019/\n",
            "Extracted ./2020.zip to ./2020/\n",
            "Extracted ./2021.zip to ./2021/\n",
            "Extracted ./2022.zip to ./2022/\n",
            "Extracted ./2023.zip to ./2023/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_author(bibrecord_data):\n",
        "    head = bibrecord_data.get(\"head\", {})\n",
        "    head_output_data = {\n",
        "        \"author_groups\": [],\n",
        "        \"correspondence\": [],\n",
        "        \"enhancement\": [],\n",
        "        \"citation_title\": head.get(\"citation-title\", \"\"),\n",
        "        \"abstracts\": head.get(\"abstracts\", \"\")\n",
        "    }\n",
        "\n",
        "    # Process author groups\n",
        "    author_groups = head.get(\"author-group\", [])\n",
        "    if isinstance(author_groups, dict):\n",
        "        author_groups = [author_groups]\n",
        "\n",
        "    for author in author_groups:\n",
        "        affi = author.get(\"affiliation\", {})\n",
        "        org = affi.get(\"organization\", [])\n",
        "        organization_names = [org.get(\"$\", \"\")] if isinstance(org, dict) else [o.get(\"$\", \"\") for o in org if isinstance(o, dict)]\n",
        "        authors_list = author.get(\"author\", [])\n",
        "        authors_list = [authors_list] if isinstance(authors_list, dict) else authors_list\n",
        "\n",
        "        for person in authors_list:\n",
        "            author_info = {\n",
        "                \"indexed-name\": person.get(\"preferred-name\", {}).get(\"ce:indexed-name\", \"\"),\n",
        "                \"seq\": person.get(\"@seq\", \"\"),\n",
        "                \"auid\": person.get(\"@auid\", \"\"),\n",
        "                \"affiliation\": {\n",
        "                    \"affiliation_id\": affi.get(\"@afid\", \"\"),\n",
        "                    \"dpt_id\": affi.get(\"@dptid\", \"\"),\n",
        "                    \"country\": affi.get(\"country\", \"\"),\n",
        "                    \"organization\": organization_names\n",
        "                }\n",
        "            }\n",
        "            head_output_data[\"author_groups\"].append(author_info)\n",
        "\n",
        "    return head_output_data"
      ],
      "metadata": {
        "id": "vcQ_60TLglS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"./2018/2018 copy/201800010\", 'r') as file:\n",
        "#     json_data = json.load(file)\n",
        "# abstracts_info = json_data.get(\"abstracts-retrieval-response\", {})[\"item\"][\"bibrecord\"]\n",
        "# bibliographic_data = process_author(abstracts_info)"
      ],
      "metadata": {
        "id": "6yeQRreHgqFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def append_network(nodes, edges, seen_pairs, seen_nodes, author_groups):\n",
        "  for i, author1 in enumerate(author_groups):\n",
        "      # Add author1 to nodes list if not already added\n",
        "      if author1['auid'] not in seen_nodes:\n",
        "          seen_nodes.add(author1['auid'])\n",
        "          nodes.append({\n",
        "              'Id': author1['auid'],\n",
        "              'Label': author1['indexed-name']\n",
        "          })\n",
        "\n",
        "      # Create edges between all pairs of authors within the same group\n",
        "      for j in range(i + 1, len(author_groups)):\n",
        "          author2 = author_groups[j]\n",
        "\n",
        "          # Avoid pairing author with themselves and ensure unique edges\n",
        "          if author1['auid'] != author2['auid']:\n",
        "              sorted_pair = tuple(sorted([author1['auid'], author2['auid']]))\n",
        "              if sorted_pair not in seen_pairs:\n",
        "                  seen_pairs.add(sorted_pair)\n",
        "                  edges.append({\n",
        "                      'Source': sorted_pair[0],\n",
        "                      'Target': sorted_pair[1],\n",
        "                      'Type': 'Undirected',  # Change to 'Directed' if needed\n",
        "                      'Weight': 1  # Modify or calculate as necessary\n",
        "                  })"
      ],
      "metadata": {
        "id": "0FwXrqxXgq8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chardet\n",
        "\n",
        "nodes = []\n",
        "edges = []\n",
        "seen_nodes = set()\n",
        "seen_pairs = set()\n",
        "\n",
        "def process_directory(base_path, nodes, edges, seen_pairs, seen_nodes):\n",
        "    for year in range(2018, 2024):  # Assuming years 2018 to 2023\n",
        "        year_path = os.path.join(base_path, str(year), f'{year} copy')\n",
        "        if os.path.isdir(year_path):\n",
        "            for file_name in os.listdir(year_path):\n",
        "                file_path = os.path.join(year_path, file_name)\n",
        "                # Detect file encoding\n",
        "                with open(file_path, 'rb') as f:  # open in binary mode\n",
        "                    raw_data = f.read()\n",
        "                    result = chardet.detect(raw_data)\n",
        "                    encoding = result['encoding']\n",
        "\n",
        "                # Read the file with detected encoding\n",
        "                with open(file_path, 'r', encoding=encoding) as file:\n",
        "                    json_data = json.load(file)\n",
        "                    abstracts_info = json_data.get(\"abstracts-retrieval-response\", {})[\"item\"][\"bibrecord\"]\n",
        "                    bibliographic_data = process_author(abstracts_info)\n",
        "\n",
        "                    append_network(nodes, edges, seen_pairs, seen_nodes, bibliographic_data['author_groups'])\n",
        "\n",
        "                    print(f'Processed {file_name}.')\n",
        "\n",
        "    # After processing all files\n",
        "    nodes_df = pd.DataFrame(nodes)\n",
        "    edges_df = pd.DataFrame(edges)\n",
        "\n",
        "    nodes_df.to_csv('nodes.csv', index=False)\n",
        "    edges_df.to_csv('edges.csv', index=False)\n",
        "\n",
        "process_directory('./', nodes, edges, seen_pairs, seen_nodes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mPmmIFUvYu9q",
        "outputId": "c6a44c94-7cef-405d-d208-c1e8b67fa9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 201800253.\n",
            "Processed 201800190.\n",
            "Processed 201800165.\n",
            "Processed 201800060.\n",
            "Processed 201800134.\n",
            "Processed 201800331.\n",
            "Processed 201800175.\n",
            "Processed 201800202.\n",
            "Processed 201800105.\n",
            "Processed 201800018.\n",
            "Processed 201800256.\n",
            "Processed 201800075.\n",
            "Processed 201800121.\n",
            "Processed 201800137.\n",
            "Processed 201800284.\n",
            "Processed 201800155.\n",
            "Processed 201800036.\n",
            "Processed 201800044.\n",
            "Processed 201800309.\n",
            "Processed 201800275.\n",
            "Processed 201800236.\n",
            "Processed 201800307.\n",
            "Processed 201800278.\n",
            "Processed 201800102.\n",
            "Processed 201800193.\n",
            "Processed 201800151.\n",
            "Processed 201800244.\n",
            "Processed 201800010.\n",
            "Processed 201800192.\n",
            "Processed 201800239.\n",
            "Processed 201800322.\n",
            "Processed 201800032.\n",
            "Processed 201800312.\n",
            "Processed 201800045.\n",
            "Processed 201800000.\n",
            "Processed 201800274.\n",
            "Processed 201800016.\n",
            "Processed 201800116.\n",
            "Processed 201800185.\n",
            "Processed 201800261.\n",
            "Processed 201800107.\n",
            "Processed 201800220.\n",
            "Processed 201800084.\n",
            "Processed 201800304.\n",
            "Processed 201800308.\n",
            "Processed 201800288.\n",
            "Processed 201800210.\n",
            "Processed 201800006.\n",
            "Processed 201800219.\n",
            "Processed 201800039.\n",
            "Processed 201800216.\n",
            "Processed 201800177.\n",
            "Processed 201800072.\n",
            "Processed 201800206.\n",
            "Processed 201800098.\n",
            "Processed 201800025.\n",
            "Processed 201800118.\n",
            "Processed 201800191.\n",
            "Processed 201800135.\n",
            "Processed 201800287.\n",
            "Processed 201800243.\n",
            "Processed 201800293.\n",
            "Processed 201800273.\n",
            "Processed 201800043.\n",
            "Processed 201800225.\n",
            "Processed 201800296.\n",
            "Processed 201800204.\n",
            "Processed 201800330.\n",
            "Processed 201800196.\n",
            "Processed 201800195.\n",
            "Processed 201800226.\n",
            "Processed 201800057.\n",
            "Processed 201800088.\n",
            "Processed 201800033.\n",
            "Processed 201800184.\n",
            "Processed 201800013.\n",
            "Processed 201800329.\n",
            "Processed 201800310.\n",
            "Processed 201800021.\n",
            "Processed 201800316.\n",
            "Processed 201800083.\n",
            "Processed 201800058.\n",
            "Processed 201800214.\n",
            "Processed 201800136.\n",
            "Processed 201800306.\n",
            "Processed 201800264.\n",
            "Processed 201800130.\n",
            "Processed 201800126.\n",
            "Processed 201800100.\n",
            "Processed 201800255.\n",
            "Processed 201800268.\n",
            "Processed 201800095.\n",
            "Processed 201800069.\n",
            "Processed 201800074.\n",
            "Processed 201800109.\n",
            "Processed 201800080.\n",
            "Processed 201800063.\n",
            "Processed 201800157.\n",
            "Processed 201800266.\n",
            "Processed 201800097.\n",
            "Processed 201800188.\n",
            "Processed 201800319.\n",
            "Processed 201800065.\n",
            "Processed 201800082.\n",
            "Processed 201800218.\n",
            "Processed 201800154.\n",
            "Processed 201800051.\n",
            "Processed 201800115.\n",
            "Processed 201800213.\n",
            "Processed 201800056.\n",
            "Processed 201800076.\n",
            "Processed 201800283.\n",
            "Processed 201800062.\n",
            "Processed 201800238.\n",
            "Processed 201800254.\n",
            "Processed 201800104.\n",
            "Processed 201800008.\n",
            "Processed 201800022.\n",
            "Processed 201800167.\n",
            "Processed 201800024.\n",
            "Processed 201800087.\n",
            "Processed 201800231.\n",
            "Processed 201800323.\n",
            "Processed 201800267.\n",
            "Processed 201800140.\n",
            "Processed 201800011.\n",
            "Processed 201800299.\n",
            "Processed 201800004.\n",
            "Processed 201800302.\n",
            "Processed 201800189.\n",
            "Processed 201800187.\n",
            "Processed 201800291.\n",
            "Processed 201800160.\n",
            "Processed 201800163.\n",
            "Processed 201800209.\n",
            "Processed 201800292.\n",
            "Processed 201800147.\n",
            "Processed 201800042.\n",
            "Processed 201800019.\n",
            "Processed 201800240.\n",
            "Processed 201800164.\n",
            "Processed 201800028.\n",
            "Processed 201800298.\n",
            "Processed 201800317.\n",
            "Processed 201800053.\n",
            "Processed 201800221.\n",
            "Processed 201800183.\n",
            "Processed 201800168.\n",
            "Processed 201800020.\n",
            "Processed 201800017.\n",
            "Processed 201800311.\n",
            "Processed 201800091.\n",
            "Processed 201800079.\n",
            "Processed 201800027.\n",
            "Processed 201800127.\n",
            "Processed 201800314.\n",
            "Processed 201800059.\n",
            "Processed 201800153.\n",
            "Processed 201800150.\n",
            "Processed 201800047.\n",
            "Processed 201800012.\n",
            "Processed 201800200.\n",
            "Processed 201800295.\n",
            "Processed 201800015.\n",
            "Processed 201800203.\n",
            "Processed 201800143.\n",
            "Processed 201800145.\n",
            "Processed 201800071.\n",
            "Processed 201800119.\n",
            "Processed 201800305.\n",
            "Processed 201800250.\n",
            "Processed 201800215.\n",
            "Processed 201800031.\n",
            "Processed 201800282.\n",
            "Processed 201800315.\n",
            "Processed 201800325.\n",
            "Processed 201800179.\n",
            "Processed 201800159.\n",
            "Processed 201800270.\n",
            "Processed 201800023.\n",
            "Processed 201800066.\n",
            "Processed 201800001.\n",
            "Processed 201800199.\n",
            "Processed 201800108.\n",
            "Processed 201800068.\n",
            "Processed 201800090.\n",
            "Processed 201800276.\n",
            "Processed 201800265.\n",
            "Processed 201800248.\n",
            "Processed 201800122.\n",
            "Processed 201800174.\n",
            "Processed 201800263.\n",
            "Processed 201800055.\n",
            "Processed 201800142.\n",
            "Processed 201800093.\n",
            "Processed 201800260.\n",
            "Processed 201800205.\n",
            "Processed 201800138.\n",
            "Processed 201800049.\n",
            "Processed 201800139.\n",
            "Processed 201800133.\n",
            "Processed 201800223.\n",
            "Processed 201800096.\n",
            "Processed 201800030.\n",
            "Processed 201800290.\n",
            "Processed 201800029.\n",
            "Processed 201800156.\n",
            "Processed 201800212.\n",
            "Processed 201800328.\n",
            "Processed 201800286.\n",
            "Processed 201800252.\n",
            "Processed 201800014.\n",
            "Processed 201800170.\n",
            "Processed 201800246.\n",
            "Processed 201800061.\n",
            "Processed 201800092.\n",
            "Processed 201800073.\n",
            "Processed 201800300.\n",
            "Processed 201800040.\n",
            "Processed 201800241.\n",
            "Processed 201800144.\n",
            "Processed 201800132.\n",
            "Processed 201800146.\n",
            "Processed 201800178.\n",
            "Processed 201800005.\n",
            "Processed 201800186.\n",
            "Processed 201800201.\n",
            "Processed 201800259.\n",
            "Processed 201800245.\n",
            "Processed 201800258.\n",
            "Processed 201800166.\n",
            "Processed 201800233.\n",
            "Processed 201800111.\n",
            "Processed 201800294.\n",
            "Processed 201800320.\n",
            "Processed 201800227.\n",
            "Processed 201800280.\n",
            "Processed 201800035.\n",
            "Processed 201800289.\n",
            "Processed 201800112.\n",
            "Processed 201800279.\n",
            "Processed 201800198.\n",
            "Processed 201800301.\n",
            "Processed 201800009.\n",
            "Processed 201800050.\n",
            "Processed 201800094.\n",
            "Processed 201800046.\n",
            "Processed 201800207.\n",
            "Processed 201800228.\n",
            "Processed 201800089.\n",
            "Processed 201800041.\n",
            "Processed 201800232.\n",
            "Processed 201800234.\n",
            "Processed 201800230.\n",
            "Processed 201800297.\n",
            "Processed 201800324.\n",
            "Processed 201800120.\n",
            "Processed 201800281.\n",
            "Processed 201800067.\n",
            "Processed 201800229.\n",
            "Processed 201800054.\n",
            "Processed 201800081.\n",
            "Processed 201800172.\n",
            "Processed 201800128.\n",
            "Processed 201800332.\n",
            "Processed 201800327.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Expecting value: line 1 column 1 (char 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d915bc229819>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0medges_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'edges.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mprocess_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseen_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseen_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-d915bc229819>\u001b[0m in \u001b[0;36mprocess_directory\u001b[0;34m(base_path, nodes, edges, seen_pairs, seen_nodes)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;31m# Read the file with detected encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                     \u001b[0mjson_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                     \u001b[0mabstracts_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"abstracts-retrieval-response\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"item\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bibrecord\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0mbibliographic_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_author\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstracts_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nodes_df = pd.DataFrame(nodes)\n",
        "edges_df = pd.DataFrame(edges)\n",
        "\n",
        "nodes_df.to_csv('nodes.csv', index=False)\n",
        "edges_df.to_csv('edges.csv', index=False)"
      ],
      "metadata": {
        "id": "13zsqGoAbhxj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}